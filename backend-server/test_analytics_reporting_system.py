#!/usr/bin/env python3
"""
Test script for Analytics and Reporting System (Task 21)
Tests all components of the analytics and reporting implementation
"""
import asyncio
import sys
import os
import json
from datetime import datetime, timedelta
from typing import Dict, Any

# Add the backend-server directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from app.core.database import get_database
from app.services.analytics_service import AnalyticsService
from app.services.ab_testing_service import ABTestingService
from app.services.performance_monitoring_service import PerformanceMonitor
from app.middleware.performance_tracking import AgentPerformanceTracker
from app.models.analytics import (
    UserActivity, JobSearchMetrics, AgentPerformanceMetrics,
    ABTestExperiment, SystemPerformanceMetrics
)

class AnalyticsReportingSystemTest:
    """Test suite for analytics and reporting system"""
    
    def __init__(self):
        self.db = next(get_database())
        self.analytics_service = AnalyticsService(self.db)
        self.ab_testing_service = ABTestingService(self.db)
        self.performance_monitor = PerformanceMonitor(self.db)
        self.agent_tracker = AgentPerformanceTracker()
        self.test_user_id = "test_user_123"
        self.test_results = []
    
    async def run_all_tests(self):
        """Run all analytics and reporting tests"""
        print("üöÄ Starting Analytics and Reporting System Tests...")
        print("=" * 60)
        
        try:
            # Test 1: User Activity Tracking
            await self.test_user_activity_tracking()
            
            # Test 2: Job Search Metrics
            await self.test_job_search_metrics()
            
            # Test 3: Agent Performance Tracking
            await self.test_agent_performance_tracking()
            
            # Test 4: A/B Testing Framework
            await self.test_ab_testing_framework()
            
            # Test 5: Performance Monitoring
            await self.test_performance_monitoring()
            
            # Test 6: Report Generation
            await self.test_report_generation()
            
            # Test 7: Real-time Metrics
            await self.test_real_time_metrics()
            
            # Print summary
            self.print_test_summary()
            
        except Exception as e:
            print(f"‚ùå Test suite failed: {str(e)}")
            return False
        
        return all(result['passed'] for result in self.test_results)
    
    async def test_user_activity_tracking(self):
        """Test user activity tracking functionality"""
        print("\nüìä Testing User Activity Tracking...")
        
        try:
            # Track various user activities
            activities = [
                ("job_search", "search_performed", {"query": "python developer"}),
                ("resume_optimization", "resume_uploaded", {"file_size": 1024}),
                ("interview_preparation", "mock_interview_started", {"role": "software engineer"}),
                ("skills_analysis", "skills_extracted", {"skills_count": 15}),
                ("networking", "contact_discovered", {"company": "TechCorp"})
            ]
            
            activity_ids = []
            for activity_type, subtype, metadata in activities:
                activity_id = await self.analytics_service.track_user_activity(
                    user_id=self.test_user_id,
                    activity_type=activity_type,
                    activity_subtype=subtype,
                    metadata=metadata,
                    duration_seconds=30,
                    success=True
                )
                activity_ids.append(activity_id)
            
            # Verify activities were tracked
            assert len(activity_ids) == 5, "Not all activities were tracked"
            
            # Get user analytics summary
            summary = await self.analytics_service.get_user_analytics_summary(
                self.test_user_id, days=1
            )
            
            assert summary["engagement_metrics"]["total_activities"] >= 5, "Activity count mismatch"
            assert len(summary["activity_summary"]) >= 5, "Activity types not tracked properly"
            
            self.test_results.append({
                'test': 'User Activity Tracking',
                'passed': True,
                'details': f"Tracked {len(activity_ids)} activities successfully"
            })
            print("‚úÖ User Activity Tracking: PASSED")
            
        except Exception as e:
            self.test_results.append({
                'test': 'User Activity Tracking',
                'passed': False,
                'error': str(e)
            })
            print(f"‚ùå User Activity Tracking: FAILED - {str(e)}")
    
    async def test_job_search_metrics(self):
        """Test job search metrics tracking"""
        print("\nüìà Testing Job Search Metrics...")
        
        try:
            # Update job search metrics
            metrics_updates = [
                {"jobs_viewed": 10, "searches_performed": 3},
                {"jobs_saved": 5, "applications_sent": 2},
                {"interviews_scheduled": 1, "interviews_completed": 1},
                {"offers_received": 1}
            ]
            
            for update in metrics_updates:
                await self.analytics_service.update_job_search_metrics(
                    self.test_user_id, update
                )
            
            # Get updated summary
            summary = await self.analytics_service.get_user_analytics_summary(
                self.test_user_id, days=1
            )
            
            job_summary = summary["job_search_summary"]
            
            assert job_summary["jobs_viewed"] >= 10, "Jobs viewed not tracked"
            assert job_summary["applications_sent"] >= 2, "Applications not tracked"
            assert job_summary["interviews_completed"] >= 1, "Interviews not tracked"
            assert job_summary["offers_received"] >= 1, "Offers not tracked"
            assert job_summary["response_rate"] > 0, "Response rate not calculated"
            
            self.test_results.append({
                'test': 'Job Search Metrics',
                'passed': True,
                'details': f"Tracked job metrics with {job_summary['response_rate']:.1f}% response rate"
            })
            print("‚úÖ Job Search Metrics: PASSED")
            
        except Exception as e:
            self.test_results.append({
                'test': 'Job Search Metrics',
                'passed': False,
                'error': str(e)
            })
            print(f"‚ùå Job Search Metrics: FAILED - {str(e)}")
    
    async def test_agent_performance_tracking(self):
        """Test AI agent performance tracking"""
        print("\nü§ñ Testing Agent Performance Tracking...")
        
        try:
            # Track performance for different agents
            agents = [
                ("job_discovery", 250, True, 4.5),
                ("skills_analysis", 180, True, 4.2),
                ("resume_optimization", 320, True, 4.7),
                ("interview_preparation", 150, False, 3.8),
                ("networking", 200, True, 4.1)
            ]
            
            for agent_name, response_time, success, satisfaction in agents:
                await self.analytics_service.track_agent_performance(
                    agent_name=agent_name,
                    response_time_ms=response_time,
                    success=success,
                    user_satisfaction_score=satisfaction
                )
            
            # Test agent performance tracker
            await self.agent_tracker.track_agent_call(
                agent_name="job_discovery",
                response_time_ms=300,
                success=True,
                metadata={"api_calls": 3}
            )
            
            # Get system performance summary
            performance_summary = await self.analytics_service.get_system_performance_summary(1)
            
            assert "agent_performance" in performance_summary, "Agent performance not tracked"
            assert len(performance_summary["agent_performance"]) > 0, "No agent metrics found"
            
            self.test_results.append({
                'test': 'Agent Performance Tracking',
                'passed': True,
                'details': f"Tracked performance for {len(agents)} agents"
            })
            print("‚úÖ Agent Performance Tracking: PASSED")
            
        except Exception as e:
            self.test_results.append({
                'test': 'Agent Performance Tracking',
                'passed': False,
                'error': str(e)
            })
            print(f"‚ùå Agent Performance Tracking: FAILED - {str(e)}")
    
    async def test_ab_testing_framework(self):
        """Test A/B testing framework"""
        print("\nüß™ Testing A/B Testing Framework...")
        
        try:
            # Create an A/B test experiment
            experiment_id = await self.ab_testing_service.create_experiment(
                name="Job Recommendation Algorithm Test",
                description="Testing new ML algorithm vs current algorithm",
                feature_name="job_recommendations",
                control_algorithm="current_algorithm_v1",
                test_algorithm="ml_algorithm_v2",
                traffic_split=0.5,
                primary_metric="click_through_rate",
                secondary_metrics=["engagement_rate", "application_rate"]
            )
            
            assert experiment_id, "Experiment creation failed"
            
            # Start the experiment
            success = await self.ab_testing_service.start_experiment(experiment_id)
            assert success, "Failed to start experiment"
            
            # Assign users to experiment groups
            test_users = ["user1", "user2", "user3", "user4", "user5"]
            assignments = {}
            
            for user_id in test_users:
                group = await self.ab_testing_service.assign_user_to_experiment(
                    user_id, experiment_id
                )
                assignments[user_id] = group
            
            # Verify assignments
            assert len(set(assignments.values())) == 2, "Users not assigned to both groups"
            
            # Record some events
            for user_id, group in assignments.items():
                await self.ab_testing_service.record_experiment_event(
                    user_id, experiment_id, "click", 1.0
                )
            
            # Get experiment results
            results = await self.ab_testing_service.get_experiment_results(experiment_id)
            
            assert results["experiment_id"] == experiment_id, "Results mismatch"
            assert results["participants"]["total"] == len(test_users), "Participant count mismatch"
            
            # Stop the experiment
            success = await self.ab_testing_service.stop_experiment(experiment_id)
            assert success, "Failed to stop experiment"
            
            self.test_results.append({
                'test': 'A/B Testing Framework',
                'passed': True,
                'details': f"Created and ran experiment with {len(test_users)} participants"
            })
            print("‚úÖ A/B Testing Framework: PASSED")
            
        except Exception as e:
            self.test_results.append({
                'test': 'A/B Testing Framework',
                'passed': False,
                'error': str(e)
            })
            print(f"‚ùå A/B Testing Framework: FAILED - {str(e)}")
    
    async def test_performance_monitoring(self):
        """Test performance monitoring system"""
        print("\n‚ö° Testing Performance Monitoring...")
        
        try:
            # Test real-time metrics collection
            metrics = await self.performance_monitor.get_real_time_metrics()
            
            assert "timestamp" in metrics, "Timestamp missing from metrics"
            assert "system" in metrics or "agents" in metrics, "No metrics collected"
            
            # Test alert checking (simulate high CPU usage)
            self.performance_monitor.metrics_buffer['system'].append({
                'timestamp': datetime.utcnow(),
                'cpu_usage': 85.0,  # Above threshold
                'memory_usage': 60.0,
                'response_time': 1500,
                'error_rate': 2.0
            })
            
            await self.performance_monitor._check_alerts()
            
            # Get performance alerts
            alerts = await self.performance_monitor.get_performance_alerts(1)
            
            # Should have at least one alert for high CPU
            high_cpu_alerts = [a for a in alerts if 'cpu' in a.get('message', '').lower()]
            
            self.test_results.append({
                'test': 'Performance Monitoring',
                'passed': True,
                'details': f"Collected metrics and generated {len(alerts)} alerts"
            })
            print("‚úÖ Performance Monitoring: PASSED")
            
        except Exception as e:
            self.test_results.append({
                'test': 'Performance Monitoring',
                'passed': False,
                'error': str(e)
            })
            print(f"‚ùå Performance Monitoring: FAILED - {str(e)}")
    
    async def test_report_generation(self):
        """Test report generation functionality"""
        print("\nüìã Testing Report Generation...")
        
        try:
            # Generate a weekly progress report
            report_id = await self.analytics_service.create_user_report(
                user_id=self.test_user_id,
                report_type="weekly_progress"
            )
            
            assert report_id, "Report generation failed"
            
            # Generate a monthly summary report
            monthly_report_id = await self.analytics_service.create_user_report(
                user_id=self.test_user_id,
                report_type="monthly_summary"
            )
            
            assert monthly_report_id, "Monthly report generation failed"
            
            # Verify reports contain expected data
            from app.models.analytics import UserReport
            
            report = self.db.query(UserReport).filter(UserReport.id == report_id).first()
            assert report, "Report not found in database"
            assert report.data, "Report data is empty"
            assert report.insights, "Report insights are empty"
            assert report.recommendations, "Report recommendations are empty"
            
            self.test_results.append({
                'test': 'Report Generation',
                'passed': True,
                'details': f"Generated 2 reports with insights and recommendations"
            })
            print("‚úÖ Report Generation: PASSED")
            
        except Exception as e:
            self.test_results.append({
                'test': 'Report Generation',
                'passed': False,
                'error': str(e)
            })
            print(f"‚ùå Report Generation: FAILED - {str(e)}")
    
    async def test_real_time_metrics(self):
        """Test real-time metrics functionality"""
        print("\n‚è±Ô∏è Testing Real-time Metrics...")
        
        try:
            # Add some mock real-time data
            self.performance_monitor.metrics_buffer['system'].append({
                'timestamp': datetime.utcnow(),
                'cpu_usage': 45.0,
                'memory_usage': 65.0,
                'response_time': 250,
                'error_rate': 1.5
            })
            
            # Add agent metrics
            for agent in ['job_discovery', 'skills_analysis', 'resume_optimization']:
                self.performance_monitor.metrics_buffer[agent].append({
                    'timestamp': datetime.utcnow(),
                    'success_rate': 95.0,
                    'response_time': 200,
                    'cpu_usage': 15.0,
                    'memory_usage': 128.0
                })
            
            # Get real-time metrics
            metrics = await self.performance_monitor.get_real_time_metrics()
            
            assert metrics, "No real-time metrics returned"
            assert "system" in metrics, "System metrics missing"
            assert "agents" in metrics, "Agent metrics missing"
            assert len(metrics["agents"]) >= 3, "Not all agent metrics present"
            
            # Verify system health status calculation
            system_status = metrics["system"].get("status")
            assert system_status in ["healthy", "warning", "critical"], "Invalid system status"
            
            self.test_results.append({
                'test': 'Real-time Metrics',
                'passed': True,
                'details': f"Retrieved real-time metrics for system and {len(metrics['agents'])} agents"
            })
            print("‚úÖ Real-time Metrics: PASSED")
            
        except Exception as e:
            self.test_results.append({
                'test': 'Real-time Metrics',
                'passed': False,
                'error': str(e)
            })
            print(f"‚ùå Real-time Metrics: FAILED - {str(e)}")
    
    def print_test_summary(self):
        """Print comprehensive test summary"""
        print("\n" + "=" * 60)
        print("üìä ANALYTICS AND REPORTING SYSTEM TEST SUMMARY")
        print("=" * 60)
        
        passed_tests = [r for r in self.test_results if r['passed']]
        failed_tests = [r for r in self.test_results if not r['passed']]
        
        print(f"‚úÖ Passed: {len(passed_tests)}/{len(self.test_results)} tests")
        print(f"‚ùå Failed: {len(failed_tests)}/{len(self.test_results)} tests")
        
        if passed_tests:
            print("\nüéâ PASSED TESTS:")
            for test in passed_tests:
                print(f"  ‚úÖ {test['test']}: {test['details']}")
        
        if failed_tests:
            print("\nüí• FAILED TESTS:")
            for test in failed_tests:
                print(f"  ‚ùå {test['test']}: {test['error']}")
        
        print("\nüìã IMPLEMENTED FEATURES:")
        features = [
            "‚úÖ User activity tracking and analytics",
            "‚úÖ Job search progress and success metrics",
            "‚úÖ AI agent performance monitoring",
            "‚úÖ A/B testing framework for algorithm optimization",
            "‚úÖ Real-time system performance monitoring",
            "‚úÖ Automated report generation with insights",
            "‚úÖ Performance alerts and health monitoring",
            "‚úÖ API performance tracking middleware",
            "‚úÖ Comprehensive analytics dashboard (frontend)",
            "‚úÖ A/B testing management interface (frontend)"
        ]
        
        for feature in features:
            print(f"  {feature}")
        
        print(f"\nüèÜ Task 21 Implementation Status: {'COMPLETED' if len(failed_tests) == 0 else 'PARTIAL'}")
        print("=" * 60)

async def main():
    """Main test execution function"""
    print("üîß Initializing Analytics and Reporting System Test Suite...")
    
    try:
        # Initialize test suite
        test_suite = AnalyticsReportingSystemTest()
        
        # Run all tests
        success = await test_suite.run_all_tests()
        
        if success:
            print("\nüéâ All tests passed! Analytics and Reporting System is working correctly.")
            return 0
        else:
            print("\n‚ö†Ô∏è Some tests failed. Please check the implementation.")
            return 1
            
    except Exception as e:
        print(f"\nüí• Test suite initialization failed: {str(e)}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)